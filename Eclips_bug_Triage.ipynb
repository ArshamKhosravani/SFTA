{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29dd3a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install transformers accelerate bitsandbytes peft trl datasets huggingface_hub jupyterlab tqdm pandas scikit-learn torch\n",
    "%pip install ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c7bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "# Authenticate to Hugging Face (for pulling DeepSeek model weights)\n",
    "login(\n",
    "    token=os.getenv(\"HUGGINGFACE_TOKEN\"),\n",
    "    add_to_git_credential=False\n",
    ")\n",
    "\n",
    "print(\"HF login successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9697646d-2cff-4f21-a8c4-fd4d847cc89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCS0289/alirezzzhp1378/DeepseekR1_Bugtriage/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    }
   ],
   "source": [
    "import os, re, torch, pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from trl import SFTTrainer\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "# Dataset definitions\n",
    "DATASETS = [\n",
    "  {\"name\": \"mozilla\", \"train\": \"data/mozilla_full_version/train_all.jsonl\", \"test\": \"data/mozilla_full_version/test_all.csv\"},\n",
    "  {\"name\": \"eclips\", \"train\": \"data/eclips_full_version/train_all.jsonl\", \"test\": \"data/eclips_full_version/test_all.csv\"}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd3becfa-9118-4dd9-a2be-2f70bb905c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prompt(issue_text: str) -> str:\n",
    "    return (\n",
    "        \"Below is a GitHub issue. Suggest the single best developer \"\n",
    "        \"(GitHub handle or email) to resolve it. Only return the identifier.\\n\\n\"\n",
    "        \"### Issue:\\n\" + issue_text + \"\\n\\n### Assignee:\"\n",
    "    )\n",
    "\n",
    "def prepare_train(example):\n",
    "    text = example[\"title\"] + \"\\n\\n\" + example[\"body\"]\n",
    "    return {\"text\": text, \"assignee\": example[\"assignee\"]}\n",
    "\n",
    "def format_for_sft(example):\n",
    "    prompt = make_prompt(example[\"text\"])\n",
    "    return {\"text\": prompt + \" \" + example[\"assignee\"]}\n",
    "\n",
    "def predict_assignee(model, tokenizer, issue_text: str) -> str:\n",
    "    prompt = make_prompt(issue_text)\n",
    "    inputs = tokenizer(prompt,\n",
    "                       return_tensors=\"pt\",\n",
    "                       truncation=True,\n",
    "                       max_length=MAX_SEQ_LENGTH-32).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=16,\n",
    "            temperature=0.1,\n",
    "            top_k=1,\n",
    "            top_p=0.0,\n",
    "            num_return_sequences=1,\n",
    "            do_sample=False,\n",
    "        )\n",
    "    decoded = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "    return decoded.split(\"### Assignee:\")[-1].strip().split()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a9b329-0ea1-4857-b5b6-8f2d4970524a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â€¢ Train records: 12884\n",
      "â€¢ Sample raw entry: {'messages': [{'role': 'system', 'content': 'You are an expert GitHub bug triager. For each incoming issue, you will read the title and description, and choose exactly one assignee (email or name). Return only the assignee, with no extra words or punctuation.'}, {'role': 'user', 'content': 'Issue to triage:\\nTitle: [1.5][compiler] Imports not resolved correctly with generics and inner interfaces\\nBody: \\nAssign to:'}, {'role': 'assistant', 'content': 'srikanth_sankaran@in.ibm.com'}]} \n",
      "\n",
      "â€¢ After mapping, columns = ['prompt', 'response']\n",
      "â€¢ Mapped sample: {'prompt': 'Issue to triage:\\nTitle: [1.5][compiler] Imports not resolved correctly with generics and inner interfaces\\nBody: \\nAssign to:', 'response': 'srikanth_sankaran@in.ibm.com'} \n",
      "\n",
      "â€¢ Ready-to-train SFT examples: 12884\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75f7e264e3a84bc98197bdb2a0baf9e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6517c4e4db4821b4d0040d73e503b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/12884 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2418' max='2418' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2418/2418 1:11:16, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.898500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.721200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.524300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.455800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.422400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.384200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.343400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.359300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.304000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.319500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.289800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.318300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.327400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.314400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.290400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.270500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.180400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.215200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.193100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.176200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.187300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.197900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.167900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.195300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.211900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.184200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.189300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.187400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.193800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.194700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.188400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.111200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.112900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.097900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.101700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.099500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.102000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.099800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.101400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.101300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.098200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.094800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.102200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.094300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.100400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.095900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved full fine-tuned model and tokenizer to: outputs/eclips_fullft/final\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    set_seed,\n",
    ")\n",
    "from typing import Dict, List\n",
    "\n",
    "DATASET     = {\n",
    "    \"name\":       \"eclips\",\n",
    "    \"train_file\": \"File_Path\",\n",
    "    \"test_file\":  \"File_Path\",\n",
    "}\n",
    "MODEL_NAME   = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "MAX_SEQ_LEN  = 2048\n",
    "LR           = 1e-5\n",
    "EPOCHS       = 3\n",
    "BATCH_PER_GPU= 1\n",
    "GRAD_ACCUM   = 16\n",
    "SEED         = 3407\n",
    "\n",
    "set_seed(SEED)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "def make_prompt(issue_text: str) -> str:\n",
    "    return (\n",
    "        \"Below is a GitHub issue. Suggest the single best developer \"\n",
    "        \"(GitHub handle or email) to resolve it. Only return the identifier.\\n\\n\"\n",
    "        \"### Issue:\\n\" + issue_text + \"\\n\\n### Assignee:\"\n",
    "    )\n",
    "\n",
    "#Load & map dataset to (prompt, response) \n",
    "raw = load_dataset(\"json\", data_files={\"train\": DATASET[\"train_file\"]}, split=\"train\")\n",
    "print(f\"â€¢ Train records: {len(raw)}\")\n",
    "print(\"â€¢ Sample raw entry:\", raw[0], \"\\n\")\n",
    "\n",
    "def extract_pair(ex):\n",
    "    msgs       = ex[\"messages\"]\n",
    "    user_msg   = next(m for m in msgs if m[\"role\"] == \"user\")[\"content\"]\n",
    "    assist_msg = next(m for m in msgs if m[\"role\"] == \"assistant\")[\"content\"]\n",
    "    return {\"prompt\": user_msg.strip(), \"response\": assist_msg.strip()}\n",
    "\n",
    "mapped = raw.map(extract_pair, remove_columns=raw.column_names)\n",
    "print(f\"â€¢ After mapping, columns = {mapped.column_names}\")\n",
    "print(\"â€¢ Mapped sample:\", mapped[0], \"\\n\")\n",
    "\n",
    "hf_train = Dataset.from_dict({\n",
    "    \"prompt\":   [row[\"prompt\"]   for row in mapped],\n",
    "    \"response\": [row[\"response\"] for row in mapped],\n",
    "})\n",
    "print(f\"â€¢ Ready-to-train SFT examples: {len(hf_train)}\\n\")\n",
    "\n",
    "#Tokenizer & Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "dtype = torch.bfloat16 if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else torch.float16\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.config.use_cache = False\n",
    "\n",
    "def tokenize_and_mask(batch: Dict[str, List[str]]):\n",
    "    prompts  = [make_prompt(p) for p in batch[\"prompt\"]]\n",
    "    answers  = batch[\"response\"]\n",
    "    full_texts = [p + \" \" + a for p, a in zip(prompts, answers)]\n",
    "\n",
    "    enc = tokenizer(\n",
    "        full_texts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        padding=False,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "    enc_prompt = tokenizer(\n",
    "        prompts,\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        padding=False,\n",
    "        add_special_tokens=True,\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for ids, p_ids in zip(enc[\"input_ids\"], enc_prompt[\"input_ids\"]):\n",
    "        plen = min(len(p_ids), len(ids))\n",
    "        lab = ids.copy()\n",
    "        lab[:plen] = [-100] * plen   \n",
    "        labels.append(lab)\n",
    "\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc\n",
    "\n",
    "tok_train = hf_train.map(tokenize_and_mask, batched=True, remove_columns=hf_train.column_names)\n",
    "\n",
    "def collate_fn(features):\n",
    "    batch_ids  = [f[\"input_ids\"] for f in features]\n",
    "    batch_mask = [f[\"attention_mask\"] for f in features]\n",
    "    batch = tokenizer.pad(\n",
    "        {\"input_ids\": batch_ids, \"attention_mask\": batch_mask},\n",
    "        padding=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    max_len = batch[\"input_ids\"].size(1)\n",
    "    labels = []\n",
    "    for f in features:\n",
    "        lab = f[\"labels\"]\n",
    "        labels.append(torch.tensor(lab + [-100] * (max_len - len(lab)), dtype=torch.long))\n",
    "    batch[\"labels\"] = torch.stack(labels)\n",
    "    return batch\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"outputs/{DATASET['name']}_fullft\",\n",
    "    per_device_train_batch_size=BATCH_PER_GPU,\n",
    "    gradient_accumulation_steps=GRAD_ACCUM,\n",
    "    num_train_epochs=EPOCHS,        \n",
    "    warmup_ratio=0.05,\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    weight_decay=0.01,\n",
    "    bf16=(dtype==torch.bfloat16),\n",
    "    fp16=(dtype==torch.float16),\n",
    "    optim=\"adamw_torch\",           \n",
    "    logging_steps=50,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=3,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "#  Train\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tok_train,\n",
    "    data_collator=collate_fn,\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "final_dir = f\"outputs/{DATASET['name']}_fullft/final\"\n",
    "trainer.save_model(final_dir)\n",
    "tokenizer.save_pretrained(final_dir)\n",
    "print(f\"âœ… Saved fine-tuned model and tokenizer to: {final_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6feb273-ce6a-4612-9a90-94b2e5fa6547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate space: 103 emails\n",
      "Built prior list with 103 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc175933414a40618706ba92000f1087",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Using checkpoint dir: outputs/eclips_fullft/final\n",
      "  â€¢ Found 4 weight shard(s), total size â‰ˆ 14.96 GB\n",
      "  â€¢ model.config._name_or_path = outputs/eclips_fullft/final\n",
      "  â€¢ dtype=torch.bfloat16, device=cuda:0\n",
      "âœ… Fine-tuned weights appear loaded from your local checkpoint.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae73fcd956d843099722eda2e950d4f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FT: prompting k=1..10:   0%|          | 0/1612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€â”€ FT MODEL: Single Top-10 prompting â†’ Hit@K from one ranking (ECLIPS) â”€â”€â”€â”€\n",
    "import os, re, json, glob\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig\n",
    "\n",
    "DATASET = {\n",
    "    \"name\":       \"eclips\",\n",
    "    \"train_file\": \"File_Path\",\n",
    "    \"test_file\":  \"File_Path\",\n",
    "}\n",
    "FINETUNED_DIR = \"FT File_Path\"\n",
    "\n",
    "TOP_K       = 10\n",
    "MAX_SEQ_LEN = 2048\n",
    "USE_PRIOR_BACKFILL = True\n",
    "\n",
    "test_df = pd.read_csv(DATASET[\"test_file\"])\n",
    "candidates = list(dict.fromkeys(test_df[\"assignee\"].dropna().tolist()))\n",
    "cand_lower2orig = {c.lower(): c for c in candidates}\n",
    "cand_set_lower  = set(cand_lower2orig.keys())\n",
    "print(f\"Candidate space: {len(candidates)} emails\")\n",
    "\n",
    "prior_order = []\n",
    "if USE_PRIOR_BACKFILL:\n",
    "    cnt = Counter()\n",
    "    try:\n",
    "        with open(DATASET[\"train_file\"], \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                y = next(m for m in obj[\"messages\"] if m[\"role\"] == \"assistant\")[\"content\"].strip()\n",
    "                cnt[y] += 1\n",
    "        freq_items = [(y, n) for y, n in cnt.items() if y in cand_lower2orig.values()]\n",
    "        freq_items.sort(key=lambda t: t[1], reverse=True)\n",
    "        prior_order = [y for y, _ in freq_items] + [c for c in candidates if c not in cnt]\n",
    "        print(f\"Built prior list with {len(prior_order)} entries.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not build prior ({e}); using alphabetical backfill.\")\n",
    "        prior_order = sorted(candidates)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(FINETUNED_DIR, use_fast=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    FINETUNED_DIR,\n",
    "    torch_dtype=(torch.bfloat16 if torch.cuda.is_available() else None),\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "model.eval()\n",
    "\n",
    "print(f\" Using checkpoint dir: {FINETUNED_DIR}\")\n",
    "assert os.path.isdir(FINETUNED_DIR), f\"Not a local directory: {FINETUNED_DIR}\"\n",
    "\n",
    "shards = sorted(glob.glob(os.path.join(FINETUNED_DIR, \"model*.safetensors\")))\n",
    "total_gb = sum(os.path.getsize(p) for p in shards) / (1024**3)\n",
    "print(f\"  â€¢ Found {len(shards)} weight shard(s), total size â‰ˆ {total_gb:.2f} GB\")\n",
    "print(f\"  â€¢ model.config._name_or_path = {getattr(model.config, '_name_or_path', 'n/a')}\")\n",
    "print(f\"  â€¢ dtype={getattr(model, 'dtype', 'n/a')}, device={model.device}\")\n",
    "\n",
    "if getattr(model.config, \"_name_or_path\", \"\") == \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\":\n",
    "    print(\"âš ï¸ Looks like the BASE repo name; double-check FINETUNED_DIR.\")\n",
    "else:\n",
    "    print(\"âœ… Fine-tuned weights appear loaded from your local checkpoint.\")\n",
    "\n",
    "def make_prompt_top10(issue_text: str) -> str:\n",
    "    k = 10\n",
    "    return (\n",
    "        f\"Below is a GitHub issue. List the TOP {k} developers (emails only) to triage it, \"\n",
    "        f\"ranked from best to worst. Use only emails known in this project; do not invent. \"\n",
    "        f\"Return EXACTLY {k} comma-separated items, unique, with no extra text.\\n\\n\"\n",
    "        \"Issue:\\n\" + issue_text + f\"\\n\\nTop {k} assignees:\"\n",
    "    )\n",
    "\n",
    "email_re = re.compile(r'[\\w\\.\\+\\-]+@[\\w\\.\\-]+\\.[A-Za-z]{2,}')\n",
    "\n",
    "def parse_emails(text: str, k: int = 10) -> list[str]:\n",
    "    found = email_re.findall(text)\n",
    "    picked, seen = [], set()\n",
    "    for em in found:\n",
    "        key = em.lower()\n",
    "        if key in cand_set_lower and key not in seen:\n",
    "            picked.append(cand_lower2orig[key])\n",
    "            seen.add(key)\n",
    "            if len(picked) == k:\n",
    "                break\n",
    "    return picked\n",
    "\n",
    "def backfill_to_n(current: list[str], n: int = 10) -> list[str]:\n",
    "    if len(current) >= n:\n",
    "        return current[:n]\n",
    "    pool = prior_order if prior_order else sorted(candidates)\n",
    "    for c in pool:\n",
    "        if c not in current:\n",
    "            current.append(c)\n",
    "        if len(current) == n:\n",
    "            break\n",
    "    if len(current) < n:\n",
    "        for c in candidates:\n",
    "            if c not in current:\n",
    "                current.append(c)\n",
    "            if len(current) == n:\n",
    "                break\n",
    "    return current[:n]\n",
    "\n",
    "gen_cfg = GenerationConfig.from_model_config(model.config)\n",
    "gen_cfg.do_sample = False\n",
    "gen_cfg.num_beams = 1\n",
    "gen_cfg.eos_token_id = tokenizer.eos_token_id\n",
    "gen_cfg.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config = gen_cfg\n",
    "\n",
    "all_top10 = []\n",
    "y_true = test_df[\"assignee\"].tolist()\n",
    "\n",
    "for row in tqdm(test_df.itertuples(index=False), total=len(test_df), desc=\"FT: prompting top-10 only\"):\n",
    "    issue = f\"Title: {row.title}\\n\\n{row.body}\"\n",
    "    prompt = make_prompt_top10(issue)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n",
    "                       max_length=MAX_SEQ_LEN-64, padding=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=200, do_sample=False, num_beams=1)[0]\n",
    "    text = tokenizer.decode(out, skip_special_tokens=True)\n",
    "    picked = parse_emails(text, k=10)\n",
    "    top10_list = backfill_to_n(picked, n=10)\n",
    "    all_top10.append(top10_list)\n",
    "\n",
    "rows, N = [], len(test_df)\n",
    "for k in range(1, TOP_K+1):\n",
    "    hits = sum(y_true[i] in all_top10[i][:k] for i in range(N))\n",
    "    rows.append({\"top_k\": k, \"n_hits\": hits, \"hit_ratio\": hits / N})\n",
    "df_hits = pd.DataFrame(rows)\n",
    "\n",
    "print(f\"\\nTotal test bugs: {N}\")\n",
    "display(df_hits.style.format({\"hit_ratio\": \"{:.3f}\"}).set_caption(\"ðŸŽ¯ FT â€” Hit@K from a single Top-10 prompt per issue\"))\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df_hits[\"top_k\"], df_hits[\"hit_ratio\"], marker=\"o\", linewidth=2)\n",
    "plt.title(\"FT â€” Hit@k from Single Top-10 Ranking\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"Hit Ratio\"); plt.xticks(range(1, TOP_K+1)); plt.ylim(0,1)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ffcf17-cd89-427f-a45d-ef8db0922060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/PCS0289/alirezzzhp1378/DeepseekR1_Bugtriage/venv/lib64/python3.9/site-packages/networkx/utils/backends.py:135: RuntimeWarning: networkx backend defined more than once: nx-loopback\n",
      "  backends.update(_get_backends(\"networkx.backends\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate space: 103 emails\n",
      "Built prior list with 103 entries.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d7d4ad4cfdf4b8e9fff09915aa192c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Base in 4-bit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4f49ccb3e994461a73db1633a4b267b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "BASE: prompting k=1..10:   0%|          | 0/1612 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# â”€â”€â”€ BASE MODEL: Single Top-10 prompting â†’ Hit@K from one ranking (ECLIPS) â”€â”€â”€\n",
    "import os, re, json\n",
    "import torch\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "from collections import Counter\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, GenerationConfig\n",
    "\n",
    "DATASET = {\n",
    "    \"name\":       \"eclips\",\n",
    "    \"train_file\": \"File_Path\",\n",
    "    \"test_file\":  \"File_Path\",\n",
    "}\n",
    "BASE_MODEL_NAME = \"deepseek-ai/DeepSeek-R1-Distill-Llama-8B\"\n",
    "\n",
    "TOP_K       = 10         \n",
    "MAX_SEQ_LEN = 2048\n",
    "USE_PRIOR_BACKFILL = True\n",
    "\n",
    "# â”€â”€ Candidates (from test split) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "test_df = pd.read_csv(DATASET[\"test_file\"])\n",
    "candidates = list(dict.fromkeys(test_df[\"assignee\"].dropna().tolist()))\n",
    "cand_lower2orig = {c.lower(): c for c in candidates}\n",
    "cand_set_lower  = set(cand_lower2orig.keys())\n",
    "print(f\"Candidate space: {len(candidates)} emails\")\n",
    "\n",
    "# â”€â”€ Prior-based backfill ordering (optional) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "prior_order = []\n",
    "if USE_PRIOR_BACKFILL:\n",
    "    cnt = Counter()\n",
    "    try:\n",
    "        with open(DATASET[\"train_file\"], \"r\", encoding=\"utf-8\") as f:\n",
    "            for line in f:\n",
    "                obj = json.loads(line)\n",
    "                y = next(m for m in obj[\"messages\"] if m[\"role\"] == \"assistant\")[\"content\"].strip()\n",
    "                cnt[y] += 1\n",
    "        freq_items = [(y, n) for y, n in cnt.items() if y in cand_lower2orig.values()]\n",
    "        freq_items.sort(key=lambda t: t[1], reverse=True)\n",
    "        prior_order = [y for y, _ in freq_items] + [c for c in candidates if c not in cnt]\n",
    "        print(f\"Built prior list with {len(prior_order)} entries.\")\n",
    "    except Exception as e:\n",
    "        print(f\"âš ï¸ Could not build prior ({e}); using alphabetical backfill.\")\n",
    "        prior_order = sorted(candidates)\n",
    "\n",
    "# â”€â”€ Model & tokenizer â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "try:\n",
    "    bnb_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=torch.float16)\n",
    "    model = AutoModelForCausalLM.from_pretrained(BASE_MODEL_NAME, quantization_config=bnb_cfg, device_map=\"auto\")\n",
    "    print(\"âœ… Base in 4-bit.\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ 4-bit failed ({e}); using bf16/fp16.\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        BASE_MODEL_NAME,\n",
    "        torch_dtype=(torch.bfloat16 if torch.cuda.is_available() else None),\n",
    "        device_map=\"auto\",\n",
    "    )\n",
    "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL_NAME, use_fast=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "model.eval()\n",
    "\n",
    "# â”€â”€ Prompt: request exactly Top-10 once â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def make_prompt_top10(issue_text: str) -> str:\n",
    "    k = 10\n",
    "    return (\n",
    "        f\"Below is a GitHub issue. List the TOP {k} developers (emails only) to triage it, \"\n",
    "        f\"ranked from best to worst. Use only emails known in this project; do not invent. \"\n",
    "        f\"Return EXACTLY {k} comma-separated items, unique, with no extra text.\\n\\n\"\n",
    "        \"Issue:\\n\" + issue_text + f\"\\n\\nTop {k} assignees:\"\n",
    "    )\n",
    "\n",
    "email_re = re.compile(r'[\\w\\.\\+\\-]+@[\\w\\.\\-]+\\.[A-Za-z]{2,}')\n",
    "\n",
    "def parse_emails(text: str, k: int = 10) -> list[str]:\n",
    "    found = email_re.findall(text)\n",
    "    picked, seen = [], set()\n",
    "    for em in found:\n",
    "        key = em.lower()\n",
    "        if key in cand_set_lower and key not in seen:\n",
    "            picked.append(cand_lower2orig[key])\n",
    "            seen.add(key)\n",
    "            if len(picked) == k:\n",
    "                break\n",
    "    return picked\n",
    "\n",
    "def backfill_to_n(current: list[str], n: int = 10) -> list[str]:\n",
    "    if len(current) >= n:\n",
    "        return current[:n]\n",
    "    pool = prior_order if prior_order else sorted(candidates)\n",
    "    for c in pool:\n",
    "        if c not in current:\n",
    "            current.append(c)\n",
    "        if len(current) == n:\n",
    "            break\n",
    "    if len(current) < n:\n",
    "        for c in candidates:\n",
    "            if c not in current:\n",
    "                current.append(c)\n",
    "            if len(current) == n:\n",
    "                break\n",
    "    return current[:n]\n",
    "\n",
    "# â”€â”€ Deterministic generation config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "gen_cfg = GenerationConfig.from_model_config(model.config)\n",
    "gen_cfg.do_sample = False\n",
    "gen_cfg.num_beams = 1\n",
    "gen_cfg.eos_token_id = tokenizer.eos_token_id\n",
    "gen_cfg.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config = gen_cfg\n",
    "\n",
    "# â”€â”€ Inference: single Top-10 per issue â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "all_top10 = []  # one list of length 10 per test row\n",
    "y_true = test_df[\"assignee\"].tolist()\n",
    "\n",
    "for row in tqdm(test_df.itertuples(index=False), total=len(test_df), desc=\"BASE: prompting top-10 only\"):\n",
    "    issue = f\"Title: {row.title}\\n\\n{row.body}\"\n",
    "    prompt = make_prompt_top10(issue)\n",
    "\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True,\n",
    "                       max_length=MAX_SEQ_LEN-64, padding=True).to(model.device)\n",
    "    with torch.no_grad():\n",
    "        out = model.generate(**inputs, max_new_tokens=200, do_sample=False, num_beams=1)[0]\n",
    "    text = tokenizer.decode(out, skip_special_tokens=True)\n",
    "\n",
    "    picked = parse_emails(text, k=10)\n",
    "    top10_list = backfill_to_n(picked, n=10)\n",
    "    all_top10.append(top10_list)\n",
    "\n",
    "# â”€â”€ Evaluation: Hit@K (k=1..10) computed from the single Top-10 ranking â”€â”€â”€â”€â”€â”€\n",
    "rows, N = [], len(test_df)\n",
    "for k in range(1, TOP_K+1):\n",
    "    hits = sum(y_true[i] in all_top10[i][:k] for i in range(N))\n",
    "    rows.append({\"top_k\": k, \"n_hits\": hits, \"hit_ratio\": hits / N})\n",
    "df_hits = pd.DataFrame(rows)\n",
    "\n",
    "print(f\"\\nTotal test bugs: {N}\")\n",
    "display(df_hits.style.format({\"hit_ratio\": \"{:.3f}\"}).set_caption(\"BASE â€” Hit@K from a single Top-10 prompt per issue\"))\n",
    "\n",
    "# â”€â”€ Plot â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(df_hits[\"top_k\"], df_hits[\"hit_ratio\"], marker=\"o\", linewidth=2)\n",
    "plt.title(\"BASE â€” Hit@k from Single Top-10 Ranking\")\n",
    "plt.xlabel(\"k\"); plt.ylabel(\"Hit Ratio\"); plt.xticks(range(1, TOP_K+1)); plt.ylim(0,1)\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cdb2034-d001-44d7-8512-7867bec6f057",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4f586a-99a1-4826-91d4-ef8769f540cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bugtriage)",
   "language": "python",
   "name": "bugtriage"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
